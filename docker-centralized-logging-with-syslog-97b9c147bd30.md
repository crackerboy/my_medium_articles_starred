Unknown markup type 10 { type: [33m10[39m, start: [33m31[39m, end: [33m40[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m29[39m, end: [33m57[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m56[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m82[39m, end: [33m99[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m37[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m65[39m, end: [33m73[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m123[39m, end: [33m140[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m56[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m50[39m, end: [33m51[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m58[39m, end: [33m66[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m86[39m, end: [33m112[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m19[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m42[39m, end: [33m59[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m3[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m111[39m, end: [33m144[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m217[39m, end: [33m223[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m231[39m, end: [33m260[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m237[39m, end: [33m272[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m340[39m, end: [33m354[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m29[39m, end: [33m38[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m3[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m29[39m, end: [33m32[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m85[39m, end: [33m91[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m114[39m, end: [33m122[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m267[39m, end: [33m278[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m284[39m, end: [33m295[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m13[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m87[39m, end: [33m95[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m105[39m, end: [33m128[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m140[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m162[39m, end: [33m171[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m50[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m3[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m45[39m, end: [33m53[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m62[39m, end: [33m88[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m90[39m, end: [33m109[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m65[39m, end: [33m82[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m114[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m56[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m19[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m25[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m15[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m22[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m360[39m, end: [33m377[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m120[39m, end: [33m137[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m155[39m, end: [33m171[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m340[39m, end: [33m354[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m358[39m, end: [33m369[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m382[39m, end: [33m388[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m423[39m, end: [33m439[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m274[39m, end: [33m292[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m91[39m, end: [33m100[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m49[39m, end: [33m57[39m }

# Centralize Your Docker Logging With Syslog

Centralize Your Docker Logging With Syslog

### The best way to understand our systems and their successes or failures is through great logging

![Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral).](https://cdn-images-1.medium.com/max/11520/0*V64LGVt9nOqvbmZk)*Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral).*

## **Background**

During one of the projects that I worked on in recent years, I had the task of integrating a centralized logging system with the applications stack we use (following a microservice architecture). The first idea that came to mind was to build an ELK (Elasticsearch, Logstash, Kibana) stack to accomplish this task. However, it turned out that this was not an option due to a limitation on the projectâ€™s resources.

As a result, I investigated all the options and ideas that would help me implement this feature without conflicting with our limitations. I ultimately came up with the following ideas:

* Collect Docker logs directly.

* Using Syslog to collect the logs.

As I already said, the first idea was to build an ELK stack for collecting the logs. However, that was not an option due to budget and resource limitations for the project.

Another idea was to forward the logs to a managed service like [logz.io](https://logz.io/). This option was also not possible because production servers have no internet accessâ€”plus the same budget limitations.

The third idea was to set up a Logrotate script that performs the following actions to generate the log files:

* Collect the Docker logs from /var/lib/docker/containers/*.

* Rename the files with the container names instead of the ids.

* Compress the log files.

<iframe src="https://medium.com/media/3a5ad9d8f2772b5f25deb46fe0504340" frameborder=0></iframe>

The second step of this idea was to set up a Cron job to copy the log files to a centralized server where the logs could be loaded and inspected.

This idea had many disadvantages and was more like reinventing the wheel. One of the most important disadvantages was that logs would be lost if a new deployment occurred and the containers were replaced.

That is why I started looking for an alternative solution that would help us collect the logs from the servers to a centralized server without losing logs. Luckily, Docker supports multiple log-drivers (see [the full list](https://docs.docker.com/config/containers/logging/configure/)). One of these log-drivers is Syslog, which is by default installed on Linux systems (no extra software needs to be installed). For this reason, I decided to implement a centralized logging system with Syslog.

## **Implementations**

For simplicity reasons only, let us assume that the infrastructure that hosts the microservices consists of the following nodes:

* Two-node Docker cluster hosting the services.

* One node for storing logs.

### Log server configurations

In order to configure the log server and make it ready for collecting logs from the Docker hosts, I had to do the following steps:

* Make sure that Syslog is installed or install it using the commands below:

    $> yum update -y
    $> sudo yum install rsyslog rsyslog-doc

* List on the correct TCP port. The line below must exist in the Syslog config file /etc/rsyslog.conf:

    $ModLoad imtcp $InputTCPServerRun 514

* Update the Journal Rate limit configurations for both Syslog and journald. The configurations below should be added to the /etc/rsyslog.conf file to avoid log loss in case of too many log messages. See [Syslogâ€™s documentation](https://www.rsyslog.com/doc/v8-stable/configuration/modules/imjournal.html) for more information.

    $imjournalRatelimitInterval 0
    $imjournalRatelimitBurst 0

* In addition, Rate limit configs need to be set to 0 in thejournald configuration file /etc/systemd/journald.conf.

    RateLimitInterval=0

By default, Syslog will store all logs in /var/log/messages. However, we would like to separate the logs based on the container name to make it easier for investigating. To configure the Syslog server to separate logs from different containers into different files, we need to perform the following steps:

* Decide and create the folder to keep all logs.

    $> mkdir /var/log/dockerlfs

* Collect the logs for the Docker daemon and save it to the disk. The below Syslog configuration rule (stored in /etc/rsyslog.d/docker_daemon.conf) tills Syslog to save all the logs that belong to a program that starts docker to the /var/log/dockerlfs/daemon.log.

<iframe src="https://medium.com/media/e3a6265729d10226b2031187aa66ed70" frameborder=0></iframe>

* Collect Docker container logs and save them to separate log files. The Syslog configuration rule below will save the logs into individual files for each of the running containers based on the container name. The rule should be stored in /etc/rsyslog.d/docker_container.con. This rule is relying on the Docker hosts to tag all the logs with container_name.

<iframe src="https://medium.com/media/6b528d166af00feff88e2528da263b45" frameborder=0></iframe>

The next step is to set up a Logrotate** **rule to rotate the logs and avoid big log files on the server. Old logs can be archived by system administrators. The rule below will rotate logs on the server daily or if the file size is more than 20MB. It will also keep only the last 30 log files for each container.

<iframe src="https://medium.com/media/876031307b474f06cd343a986334af03" frameborder=0></iframe>

As a final step for server configuration, we need to restart the services to make sure that our configs are loaded.

    $> systemctl restart rsyslog
    $> systemctl restart systemd-journald

## Configure Docker Hosts

Now that we are done with configuring the Syslog server, we can move on and start configuring the Docker hosts to forward the logs from the servers to the Syslog server.

The first thing that needs to be done is to reconfigure the Docker daemon to use the syslog log driver instead of journald and to tag the logs with the container name. To achieve this goal, we need to modify the Docker daemon configuration file that is located under /etc/docker. The daemon.json should include the following content:

<iframe src="https://medium.com/media/c7a9b1a1b27b4cf79164a623913b5443" frameborder=0></iframe>

The variable SYSLOG_SERVER_IP should be replaced by the Syslog server IP. The variable ENV_NAME should be replaced by the environment name (testing, staging, or production). With the configurations above, Docker will forward the logs directly to the Syslog server. In case the Syslog server is down or there is no valid connection to it, we may lose some logs with the configurations above.

To improve our logging system and avoid losing logs, we are going to do the following changes:

* Forward logs from Docker to the local server.

* Configure local Syslog to forward the logs to the centralized log server.

To forward logs from Docker to the local Syslog server, we simply need to remove the following line from /etc/docker/daemon.json or replace SYSLOG_SERVER_IP with 127.0.0.1:

    "syslog-address": "tcp://${SYSLOG_SERVER_IP}:514",

Once we are done with the Docker configuration, we need to restart the Docker daemon.

    $> systemctl restart docker

The next step is to update the Syslog configuration on the Docker node to be able to store Docker logs locally and forward them to the centralized server.

**syslog and journald configurations**

We need the following configuration item for journald in file /etc/systemd/journald.conf: RateLimitInterval=0.

We need the following configuration items for Syslog in the file /etc/rsyslog.conf:

    $ActionQueueFileName fwdRule1
    $ActionQueueSaveOnShutdown on
    $ActionQueueType LinkedList
    $ActionResumeRetryCount -1

    $imjournalRatelimitInterval 0
    $imjournalRatelimitBurst 0

* ActionQueueFileName:* *Adds* *unique name prefix for spool files.

* ActionQueueSaveOnShutdown:* *Save messages to disk on shutdown.

* ActionQueueType: Run asynchronously.

* ActionResumeRetryCount:* I*nfinite retries if the host is down.

With the configurations above, Syslog is going to keep retrying to send logs to the centralized server until logs are captured by the destinations server. However, until now, we have not configured the local Syslog server on the Docker hosts to forward the logs to any other servers. Therefore, all logs collected from Docker will be saved in the default file /var/log/messages.

Forwarding logs from Syslog to another server is very simple. You only need to add the following rule to the end of the /etc/rsyslog.conf file and replace SYSLOG_SERVER_IP with the valid log server IP. This rule basically checks all the logs and filters them based on the tags and the program name. If the message tags contain a tag called container_name or programname, start with docker. Then it will forward the logs to SYSLOG_SERVER_IP.

<iframe src="https://medium.com/media/cfd323d8051cdd218660970199651b7a" frameborder=0></iframe>

We can improve the rule above by keeping a copy of the logs locally on the Docker servers. With the changes below, our Syslog rule will send the logs to the remote Syslog server and it will keep a copy of the log files locally on the individual servers running Docker under /var/log/dockerlfs.

<iframe src="https://medium.com/media/aafedcfa4b14a64e43f98754a6de1444" frameborder=0></iframe>

Since we now have a copy of the log files on the Docker servers, it makes sense to add the Logrotate rule to rotate these files too. We can use the same rule used for the server nodes.

<iframe src="https://medium.com/media/876031307b474f06cd343a986334af03" frameborder=0></iframe>

Finally, a restart is needed for both Syslog and journald to pick up the new configurations.

    $> systemctl restart rsyslog
    $> systemctl restart systemd-journald

## Conclusion

Centralized logging systems and stacks can be implemented by many different methods and tools. Syslog is one of these methodsâ€”and itâ€™s especially useful when there are some limitations on the running environment and a lack of experience managing and running more sophisticated stacks like ELK.
[**Centralize Your Docker Logging With FluentD**
*Start Shipping Containers Logs to a Centralized Logging Server Using FluentD*levelup.gitconnected.com](https://levelup.gitconnected.com/centralize-your-docker-logging-with-fluentd-a2b7e0a379ce)
