Unknown markup type 10 { type: [33m10[39m, start: [33m102[39m, end: [33m116[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m120[39m, end: [33m135[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m162[39m, end: [33m170[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m199[39m, end: [33m225[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m14[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m35[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m73[39m, end: [33m85[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m95[39m, end: [33m102[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m162[39m, end: [33m181[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m414[39m, end: [33m418[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m75[39m, end: [33m92[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m122[39m, end: [33m137[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m180[39m, end: [33m195[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m137[39m, end: [33m141[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m145[39m, end: [33m150[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m223[39m, end: [33m228[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m70[39m, end: [33m85[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m167[39m, end: [33m172[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m246[39m, end: [33m251[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m261[39m, end: [33m281[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m324[39m, end: [33m328[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m362[39m, end: [33m379[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m91[39m, end: [33m98[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m41[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m39[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m6[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m159[39m, end: [33m165[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m181[39m, end: [33m211[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m10[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m36[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m107[39m, end: [33m113[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m200[39m, end: [33m218[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m4[39m, end: [33m18[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m62[39m, end: [33m77[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m329[39m, end: [33m336[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m440[39m, end: [33m449[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m6[39m, end: [33m18[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m69[39m, end: [33m73[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m79[39m, end: [33m85[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m221[39m, end: [33m228[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m6[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m3[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m159[39m, end: [33m186[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m6[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m14[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m7[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m25[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m86[39m, end: [33m100[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m277[39m, end: [33m281[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m121[39m, end: [33m136[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m59[39m, end: [33m64[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m155[39m, end: [33m172[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m343[39m, end: [33m362[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m111[39m, end: [33m114[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m128[39m, end: [33m164[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m4[39m, end: [33m18[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m23[39m, end: [33m38[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m102[39m, end: [33m126[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m84[39m, end: [33m87[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m155[39m, end: [33m172[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m185[39m, end: [33m203[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m57[39m, end: [33m80[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m41[39m, end: [33m55[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m12[39m, end: [33m27[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m8[39m, end: [33m23[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m68[39m, end: [33m92[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m54[39m, end: [33m80[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m125[39m, end: [33m129[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m11[39m, end: [33m15[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m52[39m, end: [33m55[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m78[39m, end: [33m85[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m123[39m, end: [33m168[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m176[39m, end: [33m221[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m277[39m, end: [33m300[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m137[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m217[39m, end: [33m226[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m256[39m, end: [33m264[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m56[39m, end: [33m69[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m80[39m, end: [33m89[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m93[39m, end: [33m101[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m129[39m, end: [33m148[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m73[39m, end: [33m92[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m245[39m, end: [33m262[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m250[39m, end: [33m283[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m288[39m, end: [33m305[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m173[39m, end: [33m178[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m263[39m, end: [33m267[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m84[39m, end: [33m87[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m452[39m, end: [33m459[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m532[39m, end: [33m538[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m540[39m, end: [33m545[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m547[39m, end: [33m558[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m442[39m, end: [33m457[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m531[39m, end: [33m546[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m327[39m, end: [33m354[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m516[39m, end: [33m538[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m115[39m, end: [33m137[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m299[39m, end: [33m321[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m331[39m, end: [33m354[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m393[39m, end: [33m400[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m402[39m, end: [33m414[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m416[39m, end: [33m426[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m39[39m, end: [33m70[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m44[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m102[39m, end: [33m117[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m239[39m, end: [33m250[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m116[39m, end: [33m157[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m4[39m, end: [33m18[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m23[39m, end: [33m38[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m147[39m, end: [33m181[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m4[39m, end: [33m19[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m114[39m, end: [33m155[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m249[39m, end: [33m271[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m288[39m, end: [33m330[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m5[39m, end: [33m27[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m263[39m, end: [33m285[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m96[39m, end: [33m118[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m139[39m, end: [33m161[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m45[39m, end: [33m60[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m113[39m, end: [33m117[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m235[39m, end: [33m262[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m267[39m, end: [33m293[39m }

# How to manage Terraform state

A guide to file layout, isolation, and locking for Terraform projects

![Bulkheads in the USS South Dakota. Terraform state benefits from “bulkheads” too. Image from [Wikipedia](https://en.wikipedia.org/wiki/South_Dakota-class_battleship_(1939)#/media/File:USS_South_Dakota_(BB-57)_under_construction,_1_April_1940.jpg).](https://cdn-images-1.medium.com/max/2000/1*EHeqGG4yA2SavHSji5H_QA.jpeg)*Bulkheads in the USS South Dakota. Terraform state benefits from “bulkheads” too. Image from [Wikipedia](https://en.wikipedia.org/wiki/South_Dakota-class_battleship_(1939)#/media/File:USS_South_Dakota_(BB-57)_under_construction,_1_April_1940.jpg).*

***Update, November 17, 2016**: We took this blog post series, expanded it, and turned it into a book called [Terraform: Up & Running](http://www.terraformupandrunning.com/?ref=gruntwork-blog-comprehensive-terraform)!*

***Update, July 8, 2019**: We’ve updated this blog post series for Terraform 0.12 and released the [2nd edition of Terraform: Up & Running](https://blog.gruntwork.io/terraform-up-running-2nd-edition-early-release-is-now-available-b104fc29783f)!*

This is Part 3 of the [Comprehensive Guide to Terraform](https://blog.gruntwork.io/a-comprehensive-guide-to-terraform-b3d32832baca#.b6sun4nkn) series. In Part 1, we explained [why we picked Terraform as our IAC tool of choice and not Chef, Puppet, Ansible, SaltStack, or CloudFormation](https://blog.gruntwork.io/why-we-use-terraform-and-not-chef-puppet-ansible-saltstack-or-cloudformation-7989dad2865c#.63ls7fpkq). In Part 2, we introduced the [basic syntax and features of Terraform and used them to deploy a cluster of web servers on AWS](https://blog.gruntwork.io/an-introduction-to-terraform-f17df9c6d180). In this post, we’re going to talk about how Terraform manages state and the impact that has on file layout, isolation, and locking in a Terraform project.

Here are the topics we’ll cover:

1. [What is Terraform state?](#0054)

1. [Shared storage for state files](#aeb7)

1. [Isolating state files](#784f)

1. [The terraform_remote_state data source](#7077)

You can find sample code for the examples below at: [https://github.com/gruntwork-io/intro-to-terraform](https://github.com/gruntwork-io/intro-to-terraform). Note that all the code samples are written for Terraform 0.12.x.

## What is Terraform state?

If you went through the tutorial in [Part 2 of this series](https://blog.gruntwork.io/an-introduction-to-terraform-f17df9c6d180), you may have noticed that when you ran the terraform plan or terraform apply commands, Terraform was able to find the resources it created previously and update them accordingly. But how did Terraform know which resources it was supposed to manage? You could have all sorts of infrastructure in your AWS account deployed through a variety of mechanisms (some manually, some via Terraform, some via the CLI), so how does Terraform know which infrastructure it’s responsible for?

The answer is that Terraform records information about what infrastructure it created in a [Terraform state file](https://www.terraform.io/docs/state/). By default, when you run Terraform in the folder /foo/bar, Terraform creates the file /foo/bar/terraform.tfstate. This file contains a custom JSON format that records a mapping from the Terraform resources in your templates to the representation of those resources in the real world. For example, let’s say your Terraform template contained the following:

    resource "aws_instance" "example" {
      ami           = "ami-0c55b159cbfafe1f0"
      instance_type = "t2.micro"
    }

After running terraform apply, the terraform.tfstate file will look something like this:

    {
      "version": 4,
      "terraform_version": "0.12.0",
      "serial": 1,
      "lineage": "1f2087f9-4b3c-1b66-65db-8b78faafc6fb",
      "outputs": {},
      "resources": [
        {
          "mode": "managed",
          "type": "aws_instance",
          "name": "example",
          "provider": "provider.aws",
          "instances": [
            {
              "schema_version": 1,
              "attributes": {
                "ami": "ami-0c55b159cbfafe1f0",
                "availability_zone": "us-east-2c",
                "id": "i-00d689a0acc43af0f",
                "instance_state": "running",
                "instance_type": "t2.micro",
                "(...)": "(truncated)"
              }
            }
          ]
        }
      ]
    }

Using this simple JSON format, Terraform knows that a resource with type aws_instance and name example corresponds to an EC2 Instance in your AWS account with ID i-00d689a0acc43af0f. Every time you run Terraform, it can fetch the latest status of this EC2 Instance from AWS and compare that to what’s in your Terraform configurations to determine what changes need to be applied. In other words, the output of the plan command is a diff between the code on your computer and the infrastructure deployed in the real world, as discovered via IDs in the state file.

If you’re using Terraform for a personal project, storing state in a local terraform.tfstate file works just fine. But if you want to use Terraform as a team on a real product, you run into several problems:

1. **Shared storage for state files**: To be able to use Terraform to update your infrastructure, each of your team members needs access to the same Terraform state files. That means you need to store those files in a shared location.

1. **Locking state files:** As soon as data is shared, you run into a new problem: locking. Without locking, if two team members are running Terraform at the same time, you may run into race conditions as multiple Terraform processes make concurrent updates to the state files, leading to conflicts, data loss, and state file corruption.

1. **Isolating state files: **When making changes to your infrastructure, it’s a best practice to isolate different environments. For example, when making a change in the staging environment, you want to be sure that you’re not going to accidentally break production. But how can you isolate your changes if all of your infrastructure is defined in the same Terraform state file?

In the following sections, we’ll dive into each of these problems and show you how to solve them.

## Shared storage for state files

The most common technique for allowing multiple team members to access a common set of files is to put them in version control (e.g. Git). With Terraform state, this is a **Bad Idea** for the following reasons:

1. **Manual error**: It’s too easy to forget to pull down the latest changes from version control before running Terraform or to push your latest changes to version control after running Terraform. It’s just a matter of time before someone on your team runs Terraform with out-of-date state files and as a result, accidentally rolls back or duplicates previous deployments.

1. **Locking**: Most version control systems do not provide any form of locking that would prevent two team members from running terraform apply on the same state file at the same time.

1. **Secrets**: All data in Terraform state files is stored in plain text. This is a problem because certain Terraform resources need to store sensitive data. For example, if you use the [aws_db_instance](https://www.terraform.io/docs/providers/aws/r/db_instance.html) resource to create a database, Terraform will store the username and password for the database in a state file in plain text. Storing plain-text secrets *anywhere* is a bad idea, including version control. As of May, 2019, this is an [open issue](https://github.com/hashicorp/terraform/issues/516) in the Terraform community, although there are some reasonable workarounds, as I will discuss shortly.

Instead of using version control, the best way to manage shared storage for state files is to use Terraform’s built-in support for remote backends. A Terraform *backend* determines how Terraform loads and stores state. The default backend, which you’ve been using this whole time, is the *local backend*, which stores the state file on your local disk. *Remote backends* allow you to store the state file in a remote, shared store. A number of remote backends are supported, including Amazon S3, Azure Storage, Google Cloud Storage, and HashiCorp’s Terraform Pro and Terraform Enterprise.

Remote backends solve all three of the issues listed above:

1. **Manual error**: Once you configure a remote backend, Terraform will automatically load the state file from that backend every time you run plan or apply and it’ll automatically store the state file in that backend after each apply, so there’s no chance of manual error.

1. **Locking**: Most of the remote backends natively support locking. To run terraform apply, Terraform will automatically acquire a lock; if someone else is already running apply, they will already have the lock, and you will have to wait. You can run apply with the -lock-timeout=<TIME> parameter to tell Terraform to wait up to TIME for a lock to be released (e.g., -lock-timeout=10m will wait for 10 minutes).

1. **Secrets**: Most of the remote backends natively support encryption in transit and encryption on disk of the state file. Moreover, those backends usually expose ways to configure access permissions (e.g., using IAM policies with an S3 bucket), so you can control who has access to your state files and the secrets the may contain. It would still be better if Terraform natively supported encrypting secrets within the state file, but these remote backends reduce most of the security concerns, as at least the state file isn’t stored in plaintext on disk anywhere.

If you’re using Terraform with AWS, [Amazon S3 (Simple Storage Service)](https://aws.amazon.com/s3/), which is Amazon’s managed file store, is typically your best bet as a remote
backend for the following reasons:

* It’s a managed service, so you don’t have to deploy and manage extra infrastructure to use it.

* It’s designed for [99.999999999% durability and 99.99% availability](https://aws.amazon.com/s3/details/#durability), which means you don’t have to worry too much about data loss or outages.

* It supports [encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html), which reduces worries about storing sensitive data in state files. Anyone on your team who has access to that S3 bucket will be able to see the state files in an unencrypted form, so this is still a partial solution, but at least the data will be encrypted at rest (S3 supports server-side encryption using AES-256) and in transit (Terraform uses SSL to read and write data in S3).

* It supports locking via [DynamoDB](https://aws.amazon.com/dynamodb/). More on this below.

* It supports [*versioning](http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html)*, so every revision of your state file is stored, and you can roll back to an older version if something goes wrong.

* It’s [inexpensive](https://aws.amazon.com/s3/pricing/), with most Terraform usage easily fitting into the free tier.

To enable remote state storage with S3, the first step is to create an S3 bucket. Create a main.tf file in a new folder (it should be a different folder from where you store the configurations from the previous blog post) and at the top of the file, specify AWS as the provider:

    provider "aws" {
      region = "us-east-2"
    }

Next, create an S3 bucket by using the [aws_s3_bucket](https://www.terraform.io/docs/providers/aws/r/s3_bucket.html) resource:

    resource "aws_s3_bucket" "terraform_state" {
      bucket = "terraform-up-and-running-state"

      # Enable versioning so we can see the full revision history of our
      # state files
      versioning {
        enabled = true
      }

      # Enable server-side encryption by default
      server_side_encryption_configuration {
        rule {
          apply_server_side_encryption_by_default {
            sse_algorithm = "AES256"
          }
        }
      }
    }

This code sets three arguments:

1. bucket: This is the name of the S3 bucket. Note that S3 bucket names must be *globally* unique amongst all AWS customers. Therefore, you will have to change the bucket parameter from terraform-up-and-running-state (which I already created) to your own name. Make sure to remember this name and take note of what AWS region you’re using, as you’ll need both pieces of information again a little later on.

1. versioning: This block enables versioning on the S3 bucket, so that every update to a file in the bucket actually creates a new version of that file. This allows you to see older versions of the file and revert to those older versions at any time.

1. server_side_encryption_configuration: This block turns server-side encryption on by default for all data written to this S3 bucket. This ensures that your state files, and any secrets they may contain, are always encrypted on disk when stored in S3.

Next, you need to create a DynamoDB table to use for locking. [DynamoDB](https://aws.amazon.com/dynamodb/) is Amazon’s distributed key-value store. It supports strongly-consistent reads and conditional writes, which are all the ingredients you need for a distributed lock system. Moreover, it’s completely managed, so you don’t have any infrastructure to run yourself, and it’s [inexpensive](https://aws.amazon.com/dynamodb/pricing/), with most Terraform usage easily fitting into the free tier.

To use DynamoDB for locking with Terraform, you must create a DynamoDB table that has a primary key called LockID (with this *exact* spelling and capitalization!). You can create such a table using the [aws_dynamodb_table](https://www.terraform.io/docs/providers/aws/r/dynamodb_table.html) resource:

    resource "aws_dynamodb_table" "terraform_locks" {
      name         = "terraform-up-and-running-locks"
      billing_mode = "PAY_PER_REQUEST"
      hash_key     = "LockID"

      attribute {
        name = "LockID"
        type = "S"
      }
    }

Run terraform init to download the provider code and then run terraform apply to deploy. Once everything is deployed, you will have an S3 bucket and DynamoDB table, but your Terraform state will still be stored locally. To configure Terraform to store the state in your S3 bucket (with encryption and locking), you need to add a backend configuration to your Terraform code. This is configuration for Terraform itself, so it lives within a terraform block, and has the following syntax:

    terraform {
      backend "<BACKEND_NAME>" {
        [CONFIG...]
      }
    }

Where BACKEND_NAME is the name of the backend you want to use (e.g., "s3") and CONFIG consists consists of one or more arguments that are specific to that backend (e.g., the name of the S3 bucket to use). Here’s what the backend configuration looks like for an [S3 backend](https://www.terraform.io/docs/backends/types/s3.html):

    terraform {
      backend "s3" {
        # Replace this with your bucket name!
        bucket         = "terraform-up-and-running-state"
        key            = "global/s3/terraform.tfstate"
        region         = "us-east-2"

        # Replace this with your DynamoDB table name!
        dynamodb_table = "terraform-up-and-running-locks"
        encrypt        = true
      }
    }

Let’s go through these settings one at a time:

1. bucket: The name of the S3 bucket to use. Make sure to replace this with the name of the S3 bucket you created earlier.

1. key: The file path within the S3 bucket where the Terraform state file should be written. You’ll see a little later on why the example code above sets this to global/s3/terraform.tfstate.

1. region: The AWS region where the S3 bucket lives. Make sure to replace this with the region of the S3 bucket you created earlier.

1. dynamodb_table: The DynamoDB table to use for locking. Make sure to replace this with the name of the DynamoDB table you created earlier.

1. encrypt: Setting this to true ensures your Terraform state will be encrypted on disk when stored in S3. We already enabled default encryption in the S3 bucket itself, so this is here as a second layer to ensure that the data is always encrypted.

To tell Terraform to store your state file in this S3 bucket, you’re going to use the terraform init command again. This little command can not only download provider code, but also configure your Terraform backend (and you’ll see yet another use later on too!). Moreover, the init command is idempotent, so it’s safe to run it over and over again:

    $ terraform init

    Initializing the backend...
    Acquiring state lock. This may take a few moments...
    Do you want to copy existing state to the new backend?
      Pre-existing state was found while migrating the previous "local" 
      backend to the newly configured "s3" backend. No existing state 
      was found in the newly configured "s3" backend. Do you want to 
      copy this state to the new "s3" backend? Enter "yes" to copy and 
      "no" to start with an empty state.

      Enter a value:

Terraform will automatically detect that you already have a state file locally and prompt you to copy it to the new S3 backend. If you type in “yes,” you should see:

    Successfully configured the backend "s3"! Terraform will automatically use this backend unless the backend configuration changes.

After running this command, your Terraform state will be stored in the S3 bucket. You can check this by heading over to [S3 console](https://console.aws.amazon.com/s3/) in your browser and clicking your bucket:

![](https://cdn-images-1.medium.com/max/2488/1*MakSeb8NJawpQHbQ_zXmFA.png)

With this backend enabled, Terraform will automatically pull the latest state from this S3 bucket before running a command, and automatically push the latest state to the S3 bucket after running a command. To see this in action, add the following output variables:

    output "s3_bucket_arn" {
      value       = aws_s3_bucket.terraform_state.arn
      description = "The ARN of the S3 bucket"
    }

    output "dynamodb_table_name" {
      value       = aws_dynamodb_table.terraform_locks.name
      description = "The name of the DynamoDB table"
    }

These variables will print out the Amazon Resource Name (ARN) of your S3 bucket and the name of your DynamoDB table. Run terraform apply:

    $ terraform apply

    Acquiring state lock. This may take a few moments...

    aws_dynamodb_table.terraform_locks: Refreshing state... 
    aws_s3_bucket.terraform_state: Refreshing state... 

    Apply complete! Resources: 0 added, 0 changed, 0 destroyed.

    Releasing state lock. This may take a few moments...

    Outputs:

    dynamodb_table_name = terraform-up-and-running-locks
    s3_bucket_arn = arn:aws:s3:::terraform-up-and-running-state

(Note how Terraform is now acquiring a lock before running apply and releasing the lock after!)

Now, head over to the S3 console again, refresh the page, and click the gray “Show” button next to “Versions.” You should now see several versions of your terraform.tfstate file in the S3 bucket:

![](https://cdn-images-1.medium.com/max/2494/1*DFeJ0JBm_lc96mMUJJ8qZw.png)

This means that Terraform is automatically pushing and pulling state data to and from S3 and S3 is storing every revision of the state file, which can be useful for debugging and rolling back to older versions if something goes wrong.

## Isolating state files

With a remote backend and locking, collaboration is no longer a problem. However, there is still one more problem remaining: isolation. When you first start using Terraform, you may be tempted to define all of your infrastructure in a single Terraform file or a single set of Terraform files in one folder. The problem with this approach is that all of your Terraform state is now stored in a single file, too, and a mistake anywhere could break everything.

For example, while trying to deploy a new version of your app in staging, you might break the app in production. Or worse yet, you might corrupt your entire state file, either because you didn’t use locking, or due to a rare Terraform bug, and now [all of your infrastructure in all environments is broken](https://charity.wtf/2016/03/30/terraform-vpc-and-why-you-want-a-tfstate-file-per-env/).

The whole point of having separate environments is that they are isolated from each other, so if you are managing all the environments from a single set of Terraform configurations, you are breaking that isolation. Just as a ship has bulkheads that act as barriers to prevent a leak in one part of the ship from immediately flooding all the others, you should have “bulkheads” built into your Terraform design.

There are two ways you could isolate state files:

1. **Isolation via workspaces**: useful for quick, isolated tests on the same configuration.

1. **Isolation via file layout**: useful for production use-cases where you need strong separation between environments.

Let’s dive into each of these in the next two sections.

### Isolation via workspaces

[*Terraform workspaces](https://www.terraform.io/docs/state/workspaces.html)* allow you to store your Terraform state in multiple, separate, named workspaces. Terraform starts with a single workspace called “default” and if you never explicitly specify a workspace, then the default workspace is the one you’ll use the entire time. To create a new workspace or switch between workspaces, you use the terraform workspace commands. Let’s experiment with workspaces on some Terraform code that deploys a single EC2 Instance:

    resource "aws_instance" "example" {
      ami           = "ami-0c55b159cbfafe1f0"
      instance_type = "t2.micro"
    }

Configure a backend for this instance using the S3 bucket and DynamoDB table you created earlier, but with the key value set to workspaces-example/terraform.tfstate:

    terraform {
      backend "s3" {
        # Replace this with your bucket name!
        bucket         = "terraform-up-and-running-state"
        key            = "workspaces-example/terraform.tfstate"
        region         = "us-east-2"

        # Replace this with your DynamoDB table name!
        dynamodb_table = "terraform-up-and-running-locks"
        encrypt        = true
      }
    }

Run terraform init and terraform apply to deploy this code:

    $ terraform init

    Initializing the backend...

    Successfully configured the backend "s3"! Terraform will automatically use this backend unless the backend configuration changes.

    Initializing provider plugins...

    (...)

    Terraform has been successfully initialized!

    $ terraform apply

    (...)

    Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

The state for this deployment is stored in the default workspace. You can confirm this by running the terraform workspace show command, which will tell you which workspace you’re currently in:

    $ terraform workspace show

    default

The default workspace stores your state in exactly the location you specify via the key configuration. If you take a look in your S3 bucket, you’ll find a terraform.tfstate file in the workspaces-example folder:

![](https://cdn-images-1.medium.com/max/2800/1*q8YmjOGco5QjTbYR9j9Rfw.png)

Let’s create a new workspace called “example1” using the terraform workspace new command:

    $ terraform workspace new example1
    Created and switched to workspace "example1"!

    You're now on a new, empty workspace. Workspaces isolate their state, so if you run "terraform plan" Terraform will not see any existing state for this configuration.

Now, note what happens if you try to run terraform plan:

    $ terraform plan

    Terraform will perform the following actions:

    # aws_instance.example will be created
      + resource "aws_instance" "example" {
          + ami                          = "ami-0c55b159cbfafe1f0"
          + instance_type                = "t2.micro"
          (...)
        }

    Plan: 1 to add, 0 to change, 0 to destroy.

Terraform wants to create a totally new EC2 Instance from scratch! That’s because the state files in each workspace are isolated from each other, and as you’re now in the example1 workspace, Terraform isn’t using the state file from the default workspace, and therefore, doesn’t see the EC2 Instance was already created there.

Try running terraform apply to deploy this second EC2 Instance in the new workspace:

    $ terraform apply

    (...)

    Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

Let’s repeat the exercise one more time and create another workspace called “example2”:

    $ terraform workspace new example2
    Created and switched to workspace "example2"!

    You're now on a new, empty workspace. Workspaces isolate their state, so if you run "terraform plan" Terraform will not see any existing state for this configuration.

And run terraform apply once again to deploy a third EC2 Instance:

    $ terraform apply

    (...)

    Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

You now have three workspaces available, which you can see with the terraform workspace list command:

    $ terraform workspace list
      default
      example1
    * example2

And you can switch between them at any time using the terraform workspace select command:

    $ terraform workspace select example1

    Switched to workspace "example1".

To understand how this works under the hood, take a look again in your S3 bucket, and you should now see a new folder called env:

![](https://cdn-images-1.medium.com/max/2800/1*Qf7wIqQrTcbZ7Dwfum57qQ.png)

Inside the env: folder, you’ll find one folder for each of your workspaces:

![](https://cdn-images-1.medium.com/max/2800/1*zXtEu91zDMrX208AbpPzew.png)

Inside each of those workspaces, Terraform uses the key you specified in your backend configuration, so you should find an example1/workspaces-example/terraform.tfstate and an example2/workspaces-example/terraform.tfstate. In other words, switching to a different workspace is equivalent to changing the path where your state file is stored.

This is handy when you already have a Terraform module deployed, and you want to do some experiments with it (e.g., try to refactor the code), but you don’t want your experiments to affect the state of the already deployed
infrastructure. Terraform workspaces allow you to run terraform workspace new and deploy a new copy of the exact same infrastructure, but storing the state in a separate file.

In fact, you can even change how that module behaves based on the workspace you’re in by reading the workspace name using the expression terraform.workspace. For example, here’s how to change set the instance type to t2.medium in the default workspace and t2.micro in all other workspaces (to save money when experimenting):

    resource "aws_instance" "example" {
      ami           = "ami-0c55b159cbfafe1f0"
      instance_type = (
        terraform.workspace == "default" 
        ? "t2.medium" 
        : "t2.micro"
      )
    }

The code above uses *ternary syntax* to conditionally set instance_type to either t2.medium or t2.micro, depending on the value of terraform.workspace. You’ll see the full details of conditional logic in Terraform in [Terraform tips & tricks: loops, if-statements, and pitfalls](https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9).

Terraform workspaces can be a great way to quickly spin up and tear down different versions of your code, but they have a few drawbacks:

1. The state files for all of your workspaces are stored in the same backend (e.g., the same S3 bucket). That means you use the same authentication and access controls for all the workspaces, which is one major reason workspaces are an unsuitable mechanism for isolating environments (e.g., isolating staging from production).

1. Workspaces are not visible in the code or on the terminal unless you run terraform workspace commands. When browsing the code, a module that has been deployed in one workspace looks exactly the same as a module deployed in ten workspaces. This makes maintenance harder, as you don’t have a good picture of your infrastructure.

1. Putting the two previous items together, the result is that workspaces can be fairly error prone. The lack of visibility makes it easy to forget what workspace you’re in and accidentally make changes in the wrong one (e.g., accidentally running terraform destroy in a “production” workspace rather than a “staging” workspace), and since you have to use the same authentication mechanism for all workspaces, you have no other layers of defense to protect against such errors.

To get proper isolation between environments, instead of workspaces, you’ll most likely want to use file layout, which is the topic of the next section. But before moving on, make sure to clean up the three EC2 Instances you just
deployed by running terraform workspace select <name> and terraform destroy in each of the three workspaces!

### Isolation via file layout

To get full isolation between environments, you need to:

1. Put the Terraform configuration files for each environment into a separate folder. For example, all the configurations for the staging environment can be in a folder called stage and all the configurations for the production environment can be in a folder called prod.

1. Configure a different backend for each environment, using different authentication mechanisms and access controls (e.g., each environment could live in a separate AWS account with a separate S3 bucket as a backend).

With this approach, the use of separate folders makes it much clearer which environments you’re deploy to, and the use of separate state files, with separate authentication mechanisms, makes it significantly less likely that a screw up in one environment can have any impact on another.

In fact, you may want to take the isolation concept beyond environments and down to the “component” level, where a component is a coherent set of resources that you typically deploy together. For example, once you’ve set up the basic network topology for your infrastructure—in AWS lingo, your Virtual Private Cloud (VPC) and all the associated subnets, routing rules, VPNs, and network ACLs—you will probably only change it once every few months, at most. On the other hand, you may deploy a new version of a web server multiple times per day. If you manage the infrastructure for both the VPC component and the web server component in the same set of Terraform configurations, you are unnecessarily putting your entire network topology at risk of breakage (e.g., from a simple typo in the code or someone accidentally running the wrong command) multiple times per day.

Therefore, I recommend using separate Terraform folders (and therefore separate state files) for each environment (staging, production, etc.) and for each component (vpc, services, databases). To see what this looks like in practice, let’s go through the recommended file layout for Terraform projects.

Here is the file layout for a typical Terraform project:

    stage
      └ vpc
      └ services
          └ frontend-app
          └ backend-app
              └ main.tf
              └ outputs.tf
              └ variables.tf
      └ data-storage
          └ mysql
          └ redis
    prod
      └ vpc
      └ services
          └ frontend-app
          └ backend-app
      └ data-storage
          └ mysql
          └ redis
    mgmt
      └ vpc
      └ services
          └ bastion-host
          └ jenkins
    global
      └ iam
      └ s3

At the top level, there are separate folders for each environment. The exact environments differ for every project, but the typical ones are:

* **stage**: An environment for pre-production workloads (i.e., testing).

* **prod**: An environment for production workloads (i.e., user-facing apps).

* **mgmt**: An environment for DevOps tooling (e.g., bastion host, Jenkins).

* **global**: A place to put resources that are used across all environments (e.g., S3, IAM).

Within each environment, there are separate folders for each component. The components differ for every project, but the typical ones are:

* **vpc**: The network topology for this environment.

* **services**: The apps or microservices to run in this environment, such as a Ruby on Rails frontend or a Java backend. Each app could even live in its own folder to isolate it from all the other apps.

* **data-storage**: The data stores to run in this environment, such as MySQL or Redis. Each data store could even live in its own folder to isolate it from all other data stores.

Within each component, there are the actual Terraform configuration files, which are organized according to the following naming conventions:

* **variables.tf**: Input variables.

* **outputs.tf**: Output variables.

* **main.tf**: The actual resources.

When you run Terraform, it simply looks for files in the current directory with the .tf extension, so you can use whatever filenames you want. However, while Terraform may not care about file names, your teammates probably do.
Using a consistent, predictable naming convention makes your code easier to browse, as you’ll always know where to look to find a variable, output, or resource. If individual Terraform files are becoming massive — especially
main.tf — it’s OK to break out certain functionality into separate files (e.g., iam.tf, s3.tf, database.tf), but that may also be a sign that you should break your code into smaller modules instead, a topic I’ll dive into in [How to create reusable infrastructure with Terraform modules](https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d).

This file layout makes it easy to browse the code and understand exactly what components are deployed in each environment. It also provides a good amount of isolation between environments and between components within an environment, ensuring that if something goes wrong, the damage is contained as much as possible to just one small part of your entire infrastructure.

Of course, this very same property is, in some ways, a drawback, too: splitting components into separate folders prevents you from accidentally blowing up your entire infrastructure in one command, but it also prevents you from creating your entire infrastructure in one command. If all of the components for a single environment were defined in a single Terraform configuration, you could spin up an entire environment with a single call to terraform apply. But if all the components are in separate folders, then you need to run terraform apply separately in each one (we’ll discuss some solutions to this problem later in the series).

There is another problem with this file layout: it makes it harder to use resource dependencies. If your app code was defined in the same Terraform configuration files as the database code, then that app could directly access
attributes of the database (e.g., the database address and port) using an attribute reference (e.g., aws_db_instance.foo.address). But if the app code and database code live in different folders, as I’ve recommended, you can no longer do that. Fortunately, Terraform offers a solution: the terraform_remote_state data source.

## The terraform_remote_state data source

In [Part 2, An Introduction to Terraform](https://blog.gruntwork.io/an-introduction-to-terraform-f17df9c6d180#.j79allnik), we used data sources to fetch read-only information from AWS, such as the [aws_availability_zones](https://www.terraform.io/docs/providers/aws/d/availability_zones.html) data source, which returns a list of availability zones in the current region. There is another data source that is particularly useful when working with state: [terraform_remote_state](https://www.terraform.io/docs/providers/terraform/d/remote_state.html). You can use this data source to fetch the Terraform state file stored by another set of templates.

Let’s go through an example. Imagine that your web server cluster needs to talk to a MySQL database. Running a database that is scalable, secure, durable, and highly available is a lot of work. Once again, you can let AWS take care of it for you, this time by using Amazon’s [Relational Database Service (RDS)](https://aws.amazon.com/rds/). RDS supports a variety of databases, including MySQL, PostgreSQL, SQL Server, and Oracle.

You may not want to define the MySQL database in the same set of configuration files as the web server cluster, as you’ll be deploying updates to the web server cluster far more frequently and don’t want to risk accidentally breaking the database each time you do so. Therefore, your first step should be to create a new folder at stage/data-stores/mysql and create the basic Terraform files (main.tf, variables.tf, outputs.tf) within it.

Next, create the database resources in stage/data-stores/mysql/main.tf:

    provider "aws" {
      region = "us-east-2"
    }

    resource "aws_db_instance" "example" {
      identifier_prefix   = "terraform-up-and-running"
      engine              = "mysql"
      allocated_storage   = 10
      instance_class      = "db.t2.micro"
      name                = "example_database"
      username            = "admin"
      password            = "password"
    }

At the top of the file, you see the typical provider resource, but just below that is a new resource: [aws_db_instance](https://www.terraform.io/docs/providers/aws/r/db_instance.html). This resource creates a database in RDS. The settings in this code configure RDS to run MySQL with 10GB of storage on a db.t2.micro instance, which has 1 virtual CPU, 1GB of memory, and is part of the AWS free tier.

The next step is to configure the database code to store its state in the S3 bucket you created earlier at the path stage/data-stores/mysql/terraform.tfstate:

    terraform {
      backend "s3" {
        # Replace this with your bucket name!
        bucket         = "terraform-up-and-running-state"
        key            = "stage/data-stores/mysql/terraform.tfstate"
        region         = "us-east-2"

        # Replace this with your DynamoDB table name!
        dynamodb_table = "terraform-up-and-running-locks"
        encrypt        = true
      }
    }

Run terraform init and terraform apply to create the database. Note that RDS can take ~10 minutes to provision even a small database, so be patient!

Now that you have a database, how do you provide its address and port to your web server cluster? The first step is to add two output variables to stage/data-stores/mysql/outputs.tf:

    output "address" {
      value       = aws_db_instance.example.address
      description = "Connect to the database at this endpoint"
    }

    output "port" {
      value       = aws_db_instance.example.port
      description = "The port the database is listening on"
    }

Run terraform apply one more time and you should see the outputs in the terminal:

    $ terraform apply

    (...)

    Apply complete! Resources: 0 added, 0 changed, 0 destroyed.

    Outputs:

    address = tf-2016111123.cowu6mts6srx.us-east-2.rds.amazonaws.com
    port = 3306

These outputs are now also stored in the Terraform state for the database, which is in your S3 bucket at the path stage/data-stores/mysql/terraform.tfstate. You can get the web server cluster code to read the data from this state file by adding the terraform_remote_state data 
source in _stage/services/webserver-cluster/main.tf_:

    data "terraform_remote_state" "db" {
      backend = "s3"

      config = {
        # Replace this with your bucket name!
        bucket = "terraform-up-and-running-state"
        key    = "stage/data-stores/mysql/terraform.tfstate"
        region = "us-east-2"
      }
    }

This terraform_remote_state data source configures the web server cluster code to read the state file from the same S3 bucket and folder where the database stores its state. It’s important to understand that, like all Terraform data sources, the data returned by terraform_remote_state is read-only. Nothing you do in your web server cluster Terraform code can modify that state, so you can pull in the database’s state data with no risk of causing any problems in the database itself.

All the database’s output variables are stored in the state file and you can read them from the terraform_remote_state data source using an attribute
reference of the form:

    data.terraform_remote_state.<NAME>.outputs.<ATTRIBUTE>

For example, here is how you can update the User Data of the web server cluster instances to pull the database address and port out of the terraform_remote_state data source and expose that information in the HTTP response:

    user_data = <<EOF
    #!/bin/bash
    db_address="${data.terraform_remote_state.db.outputs.address}"
    db_port="${data.terraform_remote_state.db.outputs.port}"
    echo "Hello, World. DB is at $db_address:$db_port" >> index.html
    nohup busybox httpd -f -p "${var.server_port}" &
    EOF

If you redeploy the web server cluster using terraform apply, wait for the Instances to register in the CLB, and curl the CLB URL, you’ll see something similar to:

    $ curl [http://terraform-asg-example-123.us-east-2.elb.amazonaws.com](http://terraform-asg-example-123.us-east-1.elb.amazonaws.com)

    Hello, World. DB is at tf-201911112.cowu6mts5srx.us-east-2.rds.amazonaws.com:3306

As a reminder, the sample code for the examples above is available at: [https://github.com/gruntwork-io/intro-to-terraform](https://github.com/gruntwork-io/intro-to-terraform).

## Conclusion

The reason we put so much thought into isolation, locking, and state is that infrastructure-as-code (IAC) has different trade-offs than normal coding. When you’re writing code for a typical app, most bugs are relatively minor and only break a small part of a single app. When you’re writing code that controls your infrastructure, bugs tend to be more severe, as they can break all of your apps — and all of your data stores and your entire network topology and just about everything else. Therefore, we recommend including more “safety mechanisms” when working on IAC than with typical code (for more info, see [Agility Requires Safety](http://www.ybrikman.com/writing/2016/02/14/agility-requires-safety/)).

A common concern of using the recommended file layout is that it leads to a lot of duplication. If you’re running an app called frontend-app in both staging and production, how do you avoid having to copy & paste a lot of code between stage/services/frontend-app and prod/services/frontend-app? The answer is to that you need to use Terraform modules, which is the main topic of Part 4 of the series, [How to create reusable infrastructure with Terraform modules](https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d).

*For an expanded version of this blog post series, pick up a copy of the book [Terraform: Up & Running](http://www.terraformupandrunning.com/?ref=gruntwork-blog-comprehensive-terraform) ([2nd edition available now](https://blog.gruntwork.io/terraform-up-running-2nd-edition-early-release-is-now-available-b104fc29783f)!). If you need help with Terraform, DevOps practices, or AWS at your company, feel free to reach out to us at [Gruntwork](http://www.gruntwork.io/?ref=gruntwork-blog-terraform-state).*
