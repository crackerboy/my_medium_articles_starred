Unknown markup type 10 { type: [33m10[39m, start: [33m18[39m, end: [33m23[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m311[39m, end: [33m315[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m417[39m, end: [33m445[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m68[39m, end: [33m78[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m186[39m, end: [33m219[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m258[39m, end: [33m290[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m368[39m, end: [33m374[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m437[39m, end: [33m442[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m497[39m, end: [33m512[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m82[39m, end: [33m89[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m114[39m, end: [33m121[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m244[39m, end: [33m258[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m296[39m, end: [33m303[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m65[39m, end: [33m76[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m78[39m, end: [33m89[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m91[39m, end: [33m98[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m100[39m, end: [33m107[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m175[39m, end: [33m185[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m190[39m, end: [33m197[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m281[39m, end: [33m291[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m296[39m, end: [33m303[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m130[39m, end: [33m149[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m151[39m, end: [33m161[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m163[39m, end: [33m177[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m179[39m, end: [33m193[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m94[39m, end: [33m104[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m109[39m, end: [33m116[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m65[39m, end: [33m74[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m83[39m, end: [33m90[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m96[39m, end: [33m105[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m265[39m, end: [33m283[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m365[39m, end: [33m372[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m388[39m, end: [33m398[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m16[39m, end: [33m23[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m59[39m, end: [33m65[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m129[39m, end: [33m135[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m13[39m, end: [33m41[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m60[39m, end: [33m69[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m74[39m, end: [33m80[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m152[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m161[39m, end: [33m165[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m174[39m, end: [33m183[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m195[39m, end: [33m203[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m208[39m, end: [33m216[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m220[39m, end: [33m226[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m255[39m, end: [33m262[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m267[39m, end: [33m277[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m310[39m, end: [33m321[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m71[39m, end: [33m81[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m148[39m, end: [33m157[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m185[39m, end: [33m195[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m37[39m, end: [33m45[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m50[39m, end: [33m61[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m147[39m, end: [33m154[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m170[39m, end: [33m180[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m90[39m, end: [33m97[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m112[39m, end: [33m117[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m188[39m, end: [33m217[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m430[39m, end: [33m440[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m578[39m, end: [33m597[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m599[39m, end: [33m609[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m611[39m, end: [33m625[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m632[39m, end: [33m646[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m657[39m, end: [33m667[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m672[39m, end: [33m680[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m66[39m, end: [33m70[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m24[39m, end: [33m31[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m40[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m58[39m, end: [33m66[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m144[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m174[39m, end: [33m185[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m258[39m, end: [33m269[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m278[39m, end: [33m282[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m300[39m, end: [33m308[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m332[39m, end: [33m342[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m473[39m, end: [33m483[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m488[39m, end: [33m495[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m556[39m, end: [33m588[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m31[39m, end: [33m42[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m138[39m, end: [33m148[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m17[39m, end: [33m26[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m37[39m, end: [33m47[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m67[39m, end: [33m71[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m25[39m, end: [33m32[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m41[39m, end: [33m53[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m59[39m, end: [33m67[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m146[39m, end: [33m158[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m176[39m, end: [33m187[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m261[39m, end: [33m272[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m281[39m, end: [33m285[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m303[39m, end: [33m311[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m335[39m, end: [33m345[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m450[39m, end: [33m483[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m564[39m, end: [33m574[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m580[39m, end: [33m589[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m152[39m, end: [33m163[39m }

# Kubernetes: A Detailed Example of Deployment of a Stateful Application

This piece is the second part of a two-part series

![Photo by [Ihor Dvoretskyi](https://unsplash.com/@ihor_dvoretskyi) on [Unsplash](https://unsplash.com/photos/sMB8yPFnbAE)](https://cdn-images-1.medium.com/max/12480/1*XODQ5FYsqWyqr_TR9KrxiA.jpeg)*Photo by [Ihor Dvoretskyi](https://unsplash.com/@ihor_dvoretskyi) on [Unsplash](https://unsplash.com/photos/sMB8yPFnbAE)*

In [Part One ](https://medium.com/engineering-zemoso/kubernetes-what-is-it-what-problems-does-it-solve-how-does-it-compare-with-its-alternatives-937fe80b754f)of this series, we discussed:

* What is [Kubernetes](https://kubernetes.io/)?

* What problems does it aim to solve?

* When should one choose to use Kubernetes? What alternatives are available?

In this piece, we will explore

* What are the design principles and architecture** **of Kubernetes?

* How to use Kubernetes, and a simple example.

To understand an example that describes how to deploy applications on Kubernetes, one should first have a preliminary understanding of Kubernetes architecture and objects. Thus, we will first outline the design principles and architecture of Kubernetes, followed by a brief explanation of relevant Kubernetes objects, and, finally, the example itself.

## **Design Principles and Architecture Behind Kubernetes**

Kubernetes is architected to abide by a set of design principles. To better understand why Kubernetes is architected the way it is, one should be familiar with these principles. So, let’s start our discussion there.

### Design principles of Kubernetes

* **Portable: **Kubernetes can run anywhere. Kubernetes runs with consistent behavior across various environments — public cloud, private cloud, on-premise or personal laptop. Applications deployed on Kubernetes can be ported across different environments with minimal effort.

* **General-purpose: **Kubernetes doesn’t put any restrictions on what type of applications can be deployed through it. Although it focuses on deployment and management of micro-services and cloud-native applications, any type of workload (batch jobs, stateless or stateful services, legacy monolithic single instance applications) can be deployed through Kubernetes. Applications could be written in any language or framework without any restrictions.

* **Flexible: **Kubernetes allows for many parts of its functionality to be substituted with custom, built-in solutions. This gives the ability to use a specialized solution along with Kubernetes wherever necessary. To ensure this flexibility, Kubernetes is built as a collection of pluggable components and layers.

* **Extensible: **Kubernetes facilitates the addition of specialized capabilities whenever necessary. This is achieved by exposing interfaces, which could be implemented to add new functionality on top of existing functionality. This allows for [numerous add-ons](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) to be developed for Kubernetes.

* **Automatable:** Kubernetes aims to reduce the burden of manual operations. Once configured, applications deployed through Kubernetes will scale and self heal without any manual intervention. Kubernetes could be integrated with a Continuous Integration (CI) pipeline, allowing a code change committed by a developer to be deployed onto the test environment automatically.

Each of these principles adds great value to the end user who is using Kubernetes. Portability allows for reliable testing of the application on various environments, such as testing and production, and prevents getting locked in with a single cloud-provider or vendor.

General purpose gives developers the freedom to choose the exact development tools and frameworks necessary to meet the business functionality, without worrying about the infrastructure or deployment.

Flexibility and extensibility allow the addition of customized functionality wherever the built-in functionality is not sufficient.

Automatability ensures that manual work necessary for the maintenance of a large-scale application is kept a minimum. This allows for a relatively small team to successfully maintain a large-scale, distributed application deployed on the cloud.

Let’s now discuss the Kubernetes architecture which was developed keeping these principles in consideration

### Architecture of Kubernetes

High-level Kubernetes comprises of the master system and workers. The master system controls the workers and runs applications on them. The desired state of the cluster (compute resources) is represented as abstract objects. These abstract Kubernetes objects are *records of intent.* Kubernetes will constantly work to ensure that the state represented in these abstract objects is the actual physical state of the cluster. An external client could connect to the master and obtain information about the cluster state and issue commands to change it as per requirement.

Whenever one wishes to update the physical state of the cluster, all they would have to do is update the abstract Kubernetes objects, and Kubernetes will take care of the rest. Let’s dive deeper and briefly discuss the components of the master system and workers.

### Components of Kubernetes master

The Kubernetes master system, also known as the control pane, is designed as a set of components. Let’s briefly discuss it’s key components.

* **API server:** Kubernetes mostly uses REST API for internal and external communication. All the abstract Kubernetes objects are exposed as REST resources. API server is the component that is responsible for processing the REST requests, validating them, and performing appropriate CRUD operations on corresponding abstract Kubernetes objects.

* **Cluster State Store: **To perform the CRUD operations, API server would need a backing data store. As the name indicates, cluster state store is a persistent storage instance which stores the state of all the abstract Kubernetes objects configured in the system. The cluster state store has support for watch functionality. Through this functionality, all the coordinating components could be quickly notified whenever a change is made to an object.

* **Controller Manager:** This is the component of master that runs controllers. Controllers run loops and monitor the actual cluster state and state represented in the abstract Kubernetes objects. Whenever a change to cluster state is notified, they are responsible for performing necessary actions, such that the actual state and the abstract state are consistent with each other. Kubernetes has numerous controllers, each one responsible for a different set of Kubernetes objects.

* **Scheduler: **It is the component of the master responsible for** **allocating physical resources on the cluster to run applications/jobs added to the abstract data store. These scheduling decisions are made taking into account numerous factors like hardware/software constraints, among others.

A Kubernetes master system could have multiple replicas of each of these components to ensure high availability, and could be deployed along with worker node components on a single physical instance. However, for simplicity, setup scripts typically start all master components on the same machine, and do not run any worker instances on this machine.

The exact cluster setup is dependent on the requirements of the end user. For smaller applications, a single instance with both master and worker components is more than sufficient. For larger applications customized effort is essential for configuring the Kubernetes cluster.

### Components of Kubernetes worker

The worker instances, or nodes, are also composed of multiple components. The main function of Kubernetes worker components is to process the instructions from master and execute them on the node. The following are the key components of a worker node:

* **Kubelet: **It is the component of worker responsible for making sure that the containers scheduled by the master on this node are running and are healthy.

* **Container runtime:** Container runtime is the software that is responsible for running containers. Kubernetes supports several runtimes and any implementation of the [Kubernetes CRI](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/). Each worker node uses this to run the containerized applications scheduled by the master. Running non-containerized applications is discouraged and not supported by Kubernetes.

* **Kube proxy:** It is the component of Worker responsible for maintaining network rules on the worker and performing connection forwarding. This essentially enables efficient and effective communication throughout the cluster. External application traffic will get redirected to the appropriate container through these components.

### **External Kubernetes client**

Theoretically, external Kubernetes client could be any application that can communicate with API server through the well-defined REST API. But the most predominant choice is to use [Kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)*.*

Kubectl is a command line tool that is intended to be used by an end-user responsible for managing application deployments. Kubectl users can execute commands on a terminal. Each of these commands is converted into an API call in the background and sent to the API server on Kubernetes master, where necessary action will be performed.

![High-Level Kubernetes Architecture](https://cdn-images-1.medium.com/max/2000/1*qC23lB5QHvlK88WorDzqLg.png)*High-Level Kubernetes Architecture*

Let’s take a step back and look at the overall architecture of Kubernetes. One could notice that it is designed as a set of loosely coupled components working together, instead of a single monolithic instance being responsible for all the functionality. We already discussed the various advantages of such an architectural style. In particular, this choice allows Kubernetes to stay f*lexible* and e*xtensible.*

The choice to use rest to create and update the cluster configuration ensures that any configuration created on one environment will work on any other environment. This allows application deployments created on Kubernetes to remain *portable.*

Controller manager and scheduler act as components which continuously watch for changes to the abstract objects in cluster state store. They send instructions to worker nodes whenever necessary to automatically update the actual state of cluster This design choice eliminates a great deal of manual work and ensure Kubernetes is a*utonomic*. In fact, this declarative approach to cluster management was one of the main features which lead to the rapid adoption of Kubernetes.

The choice to run only containerized applications by interacting with the container runtime through an interface ensures that any type of application could run on Kubernetes, and allows Kubernetes to remain g*eneral purpose*.

We can now look into the abstract objects used to represent and manage the cluster state in Kubernetes. Knowledge of these Kubernetes objects is the final piece of the puzzle we need to understand before we dive into the example

## **Kubernetes Objects**

Kubernetes defines a [large number of abstract objects](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/). For brevity’s sake, we will only discuss those Kubernetes objects that are absolutely essential for understanding our example.

* **Pod: **We know that through Kubernetes, we could run containerized applications. Instead of abstracting a single container as a Kubernetes object, Kubernetes defines *pod, *which is a group of one or more containers. There is an advantage that comes with this choice. For simpler cases, each pod in the system could represent a single container. But, whenever there is a need to deploy additional capabilities that are not directly related to the core business functionality of the container — like support for logging, caching, etc — we have an option to package these additional capabilities into separate containers and place them in a single pod. This ensures they always stay logically together. Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. It is the place where the actual application code implemented by the end-user runs. Each pod has it’s own IP address and is completely decoupled from the host.

* **Service: **In Kubernetes,** **pods are volatile. To ensure high availability and optimum use of compute resources, Kubernetes could dynamically kill and create pods. Because of this, the IP address of a pod is not a reliable way to access business functionality offered by the pod. Instead, Kubernetes recommends using a *service *to access the business functionality. Kubernetes service is an abstraction which defines a logical set of pods and a policy to access them. Every Kubernetes service has an IP address, but unlike the IP address of a pod, it is stable. A Kubernetes service continuously keeps track of all the pods in the system, and identifies the pods it is expected to target. Whenever a request to access a particular business functionality reaches the service, it will redirect the request to the IP address of one of the pods that are active in the system at that point in time. Ideally, to access the pods from outside the cluster, one must use [*Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/).* As of now, however, the Kubernetes *Ingress *feature is still beta.* *Thus, in this example, we will use a service to expose the traffic externally as well.

* **Persistent- Volume and Persistent-Volume Claim: **Managing storage is a distinct problem from managing compute. Kubernetes defines two key abstractions to handle this problem, p*ersistent volume, *and p*ersistent volume Claim*. In Kubernetes,** **a persistent-volume is a piece of storage in the cluster that has been provisioned to be used by the cluster for its storage requirements. A persistent-volume claim is a request by an application to consume the abstract storage resources declared through persistent volume. To make persistent storage available to the applications running inside Kubernetes, one should first declare persistent volume and then configure the application to make a claim to use that volume.

* **ConfigMap: ***Configmap *is a Kubernetes abstraction meant to decouple environment-dependent application-configuration-data from containerized applications, allowing them to remain portable across environments.

* **Secrets: **A *secret *is an object that contains a small amount of sensitive data such as a password, a token, or a key. Putting such sensitive information in a secret allows for more control over how it is used and reduces the risk of accidental exposure.

* **Deployment: ***Deployment* is an abstraction meant to represent the desired state of an actual deployment on Kubernetes. A deployment object typically contains all the information required — the location to obtain and build containerized applications, configuration of pods expected to package and run these containers,the number of replicas of each pod that should be maintained, the location of application configuration in terms of config-maps and secrets meant to be used by the containers, configuration of data storage (if the application needs persistent data storage). All of these could be declared inside deployment. Although it is possible to create individual pods and services in Kubernetes it is recommended that one use deployment to manage deployments. By using the deployment object, typical operations like roll-out, roll-back, and monitoring are greatly simplified.

## **How To Use Kubernetes, and a Simple Example**

Now that we have gone through the basics of Kubernetes, we will take a detailed look at a simple example. In this example, we will deploy a web application using Kubernetes. We will be using [Docker](https://www.docker.com/) as our container runtime. The application in our example has three distinct parts:

* Database (MySQL server)

* Back end (Java Spring Boot application)

* Front end (Angular app)

We will deploy all of these components on a Kubernetes cluster. We will have one replica of database, two replicas of back end* *and two replicas of front end*. *Front-end instances will communicate with back end through HTTP. Back-end instances will communicate with the database. To facilitate this communication, we have to configure Kubernetes accordingly.

We will configure the cluster by creating Kubernetes objects. These Kubernetes objects will contain the desired state of our deployment. Once these objects are persisted into the cluster state store, the internal architecture of Kubernetes will take necessary steps to ensure that the abstract state in the cluster state store is the same as the physical state of the cluster.

We will use [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) to create the objects. Kubernetes supports both imperative and declarative ways of creating objects. Production environments are generally configured by the declarative approach. We will use the declarative approach in this example. For each object, we will first prepare a manifest file, a yaml file containing all the information related to the object. Then we will execute the kubectl command, kubectl apply -f <FILE_NAME>to persist the object in the cluster state store.

We will first containerize the application code we have implemented. After this, we will configure the deployment of our database followed by back end. We will finish the example by configuring the front end.

### Step 1. Containerize the application and upload image to container image registry

The first step would be to create a *container image* of the application we have implemented and upload it to *container registry*.

A container image is a packaged form of the containerized application. It can be transferred across computers, just like any normal file. The container runtime environment can create a running instance of a containerized application using the container image.

Container registry is generally the centralized repository where container images are stored. One could upload container images to a container registry and download them wherever and whenever they are needed. There are numerous container registry services available: [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/), [Google Container ](https://cloud.google.com/container-registry/)[Registry](https://cloud.google.com/container-registry/), [Amazon ECR](https://aws.amazon.com/ecr/), etc. We will use[ Docker hub](https://hub.docker.com/) for this example, but one could use any image registry (public or private) that fits their use case.

The application we are going to deploy has front end implemented with [Angular Framework](https://angular.io/), and back end implemented with [Spring Boot Framework](https://spring.io/projects/spring-boot). Links to GitHub repositories containing the code are provided in the final section of this piece. Once we have implemented the code as per our requirements, we will build executables with build tools ([Angular CLI ](ttps://cli.angular.io)and [Maven](https://maven.apache.org/) in this case).

Now we will create container images by [building docker images](https://docs.docker.com/engine/reference/commandline/image_build/) using [Dockerfil](https://docs.docker.com/engine/reference/builder/)e . Dockerfile for both front end and back end used in this example are shown below.

<iframe src="https://medium.com/media/1bcb694b3e08a18177f1e7ede9ca7a32" frameborder=0></iframe>

<iframe src="https://medium.com/media/3078bd1bc1033bfed34fb742b45da864" frameborder=0></iframe>

Once container images are created, we can upload these to any container image registry. Here we will upload these images to Docker Hub. We have uploaded the front-end image with the name[kubernetesdemo/to-do-app-frontend](https://hub.docker.com/r/kubernetesdemo/to-do-app-frontend), and the back-end image with the name [kubernetesdemo/to-do-app-backend](https://hub.docker.com/r/kubernetesdemo/to-do-app-backend). We will obtain the database image from the official MySQL docker repository [mysql](https://hub.docker.com/_/mysql). Official Docker images generally do not have any prefix, like mysql. Unofficial images are required to have a prefix like kubernetesdemo/here.

We have to mention the name of these images in the Kubernetes manifest files, which we will see below. Kubernetes will fetch and run these images on respective cluster nodes whenever required.

### **Step 2. Set up Kubernetes cluster and CLI**

There are numerous solutions available for setting up a Kubernetes cluster. Different Kubernetes solutions meet different requirements: ease of maintenance, security, control, available resources, and expertise required to operate and manage a cluster. One could refer to the [official documentation](https://kubernetes.io/docs/setup/) for more details about how a cluster could be set up. This example has been replicated on both local ([Minikube](https://kubernetes.io/docs/setup/minikube/)) and cloud provider ([GKE](https://cloud.google.com/kubernetes-engine/)) setup. [K](https://github.com/kubernetes/kops)[ops](https://github.com/kubernetes/kops) is a project that aims to simplify the Kubernetes cluster setup process.

As mentioned earlier, we will use kubectl as our CLI. Instructions for installing kubectl can be found [here](https://kubernetes.io/docs/tasks/tools/install-kubectl/). Once kubectl is installed, it should be configured to communicate with the Kubernetes cluster we have set up. In the case of Minikube, minikube start command will automatically configure kubectl. For cloud setup, instructions can be found in their respective quick start guide(eg: [GKE](https://cloud.google.com/kubernetes-engine/docs/quickstart)).

### Step 3. Database configuration setup

Back-end instances need to communicate with the database. All the configuration details required to connect with the database are stored in a configuration file.

Let’s take a look at the back-end spring configuration file in this example

<iframe src="https://medium.com/media/acdc20dc21980b594dfef10227e8063f" frameborder=0></iframe>

This configuration file expects some environment variables, like DB_USERNAME, DB_PASSWORD, DB_HOST, DB_NAME . We will pass the values of these variables to Kubernetes through configMaps and secrets. Then we will configure the back-end pod to read the environment variable from the configMaps and secrets .

The MySQL Database docker image [expects some environment variables](https://docs.docker.com/samples/library/mysql/#environment-variables). We will need to configure the following environment variables MYSQL_ROOT_PASSWORD, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE.

Now that we have an idea about the configuration required for our application, we will create configMaps and secrets in our Kubernetes cluster with required data.

First, to hold database specific information, we will create one configMap and two secrets. The configMap will contain non-sensitive information about the database setup, like the location where the database is hosted and the name of the database. We will define a Kubernetes service, this will expose the location of the database. [Kuberebetes DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) will resolve the service name to actual ip address of the database during runtime. Below is the configMap, which is used to store non-sensitive information related to the database in this example

<iframe src="https://medium.com/media/d6ce31dcd072be8065d3a9c76cf1cc5d" frameborder=0></iframe>

We will use two secrets to store sensitive data. The first secret will contain the database root user credentials and the second secret will contain application user credentials. Below are these two files

<iframe src="https://medium.com/media/b9561f5559a89ec54c618ff2ff78d95d" frameborder=0></iframe>

<iframe src="https://medium.com/media/0b4567c3b14c11207becf33cc39b2195" frameborder=0></iframe>

By executing kubectl apply -f <FILE_NAME>we will create the ConfigMap and Secret objects in our Kubernetes cluster state store. We stored the values of host and name in this ConfigMap object and username and password in Secret object. We will access thesesecrets and ConfigMaps in later steps to configure our deployments .

Similarly, the configuration of front end expects environment variable SERVER_URI which will indicate where back end is hosted. We will create this configMap after configuring back-end Deployment

### Step 4. Configure PVC, service, and deployment for database

Our next step would be to create the services and deployments required for our database setup. Below is the file which creates relevant Kubernetes Service and Kubernetes Deployment for the database setup in this application.

<iframe src="https://medium.com/media/e5f7cfd369d40c7b0ca6e466847c9057" frameborder=0></iframe>

Through this file, we created multiple Kubernetes objects. First, we created a Kubernetes Service with the name mysql for accessing the pod running the MySQL container. Next, we created a Persistent Volume Claim (PVC) of one GB, this will result in [Kubernetes cluster dynamically allocating](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/) the required persistent storage for MySQL (enable default dynamic storage, if it is not enabled in your cluster). After this, we created a Deployment object, which configures the deployment of MySQL Server in the cluster. Into the MySQL container, we injected environment variables like MYSQL_ROOT_PASSWORD, MYSQL_USER, MYSQL_PASSWORD , and MYSQL_DATABASE using the configMaps and services we created in the previous step.

### Step 5. Configure service and deployment for back end

Next, we set up our back-end application deployment. Below is the yaml file which creates the required Kubernetes objects.

<iframe src="https://medium.com/media/9dd3f844eb39659459f9df2337fcddcf" frameborder=0></iframe>

Here we first created a Service of type LoadBalancer (use NodePort if you are running Kubernetes locally) which exposes the back-end instances. Loadbalancer type provides an External-IP , through which one could access the back-end services externally. (use minikube ipwith the port if you are using minikube). Next, we created the Deployment object configured to contain two replicas of the back-end instance. And then injected the required environment variables from the configMaps and secrets we have created earlier. This deployment will use the image kubernetesdemo/to-do-app-backend which we created in step one.

### Step 6. Front-end configuration setup

Front end expects the value of External-IP of back end, generated in the above step, to be passed in the form of the environment variable SERVER_URI. We will now create a config map to store this information related to the back-end setup.

<iframe src="https://medium.com/media/156d47099d7b8370d6abb4d0249d2d64" frameborder=0></iframe>

We will use this configMap to inject SERVER_URI value when configuring the deployment of front end, in the next step.

### Step 7. Configure service and deployment for front end

Next, we set up our front-end application deployment. Below is the yaml file which creates the required Kubernetes objects.

<iframe src="https://medium.com/media/6b9357f501e263436a3db46cf02a75f5" frameborder=0></iframe>

Here, we first created a Service of type LoadBalancer (use NodePort if you are running Kubernetes locally) which exposes the front-end instances. Loadbalancer type provides an External-IP , through which one could access the front-end services externally. (use minikube ipwith the port if you are using minikube). Next, we created the Deployment object configured to contain two replicas of the front-end instance. This deployment will use the image kubernetesdemo/to-do-app-frontend, which we created in step one. After this, we injected the environment variable SERVER_URI from configMap, which we created in the above setup.

That’s it. Our simple application is now completely deployed. After this, the front end of the application should be accessible using front-end service External-IP from any browser. The Angular app will call the back end through HTTP and back end will communicate with MySQL database, where our application data is persisted. The image below shows the overall setup described in this example

![High-Level View of To-Do-App Deployment Setup on Kubernetes](https://cdn-images-1.medium.com/max/2000/1*78_FSXLxWFY8eOQS-zJ8qA.png)*High-Level View of To-Do-App Deployment Setup on Kubernetes*

This entire deployment is now managed by Kubernetes. If one of the pods goes down for unknown reasons, Kubernetes will bring up a new pod without any manual intervention. Using [kubectl, ](https://kubernetes.io/docs/reference/kubectl/overview/)we could monitor and update this deployment, whenever required.

## External Links

* The GitHub Repository Containing the manifest files used for deployment and configuration of Kubernetes Cluster discussed in this piece can be found [here](https://github.com/shri-kanth/kuberenetes-demo-manifests).

* The GitHub Repository Containing the back-end implementation discussed in this piece can be found [here](https://github.com/shri-kanth/kubernetes-demo-backend).

* The GitHub Repository Containing the front-end implementation discussed in this piece can be found [here](https://github.com/shri-kanth/kubernetes-demo-frontend).
