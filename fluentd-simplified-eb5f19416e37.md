Unknown markup type 10 { type: [33m10[39m, start: [33m18[39m, end: [33m70[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m17[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m20[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m39[39m, end: [33m46[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m100[39m, end: [33m111[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m13[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m48[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m78[39m, end: [33m89[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m96[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m3[39m, end: [33m10[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m3[39m, end: [33m15[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m141[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m16[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m165[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m4[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m25[39m, end: [33m38[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m4[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m10[39m, end: [33m19[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m35[39m, end: [33m52[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m7[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m13[39m, end: [33m20[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m99[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m18[39m, end: [33m23[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m192[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m10[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m66[39m, end: [33m73[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m21[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m47[39m, end: [33m51[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m73[39m, end: [33m77[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m121[39m, end: [33m137[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m13[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m40[39m, end: [33m46[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m134[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m19[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m7[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m17[39m, end: [33m23[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m142[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m10[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m20[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m188[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m72[39m, end: [33m92[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m1220[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m65[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m81[39m, end: [33m103[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m112[39m, end: [33m125[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m243[39m, end: [33m261[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m27[39m, end: [33m31[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m106[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m70[39m, end: [33m88[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m10[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m8[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m53[39m, end: [33m56[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m509[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m26[39m, end: [33m30[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m107[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m68[39m, end: [33m86[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m22[39m, end: [33m28[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m26[39m, end: [33m29[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m66[39m, end: [33m77[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m0[39m, end: [33m207[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m125[39m, end: [33m128[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m133[39m, end: [33m139[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m144[39m, end: [33m150[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m152[39m, end: [33m156[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m158[39m, end: [33m162[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m164[39m, end: [33m170[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m172[39m, end: [33m176[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m178[39m, end: [33m182[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m184[39m, end: [33m188[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m190[39m, end: [33m197[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m199[39m, end: [33m204[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m209[39m, end: [33m229[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m305[39m, end: [33m315[39m }
Unknown markup type 10 { type: [33m10[39m, start: [33m47[39m, end: [33m53[39m }

# Fluentd — Simplified



If your apps are running on distributed architectures, you are very likely to be using a centralized logging system to keep their logs. The most widely used data collector for those logs is [fluentd](https://www.fluentd.org/). In this post we are going to explain how it works and show you how to tweak it to your needs. We are assuming that there is a basic understanding of docker and linux for this post.

## Basic concepts

Have you ever run tail -f myapp.log | grep "what I want" > example.log in bash? Well this is what fluentd does pretty well, tailing logs or receiving data of some form, filtering it or transforming it and then sending it to another place.

That line above can be broken in multiple pieces:

## Input

    tail -f myapp.log

Here we are tailing a file forever, every time something is added to the file it will be shown in the screen. This is called [input plugin](https://docs.fluentd.org/input) in fluentd, [tail](https://docs.fluentd.org/input/tail) is one of them, but there are many more.

## Filtering

    | grep "what I want"

Here we are taking the output from the tail -f and filtering out only lines that contain the string what I want. In fluentd-land this is called a [filter plugin](https://docs.fluentd.org/filter).

## Output

    > example.log

Here we are saving the filtered output from the grep command to a file called example.log. In fluentd this is called [output plugin](https://docs.fluentd.org/output). Besides writing to files fluentd has many plugins to send your logs to other places.

And that’s the gist of fluentd, you can read stuff, process it and send it to another place for further analysis. Let’s put these concepts into practice with a small demo to see how these 3 plugin types work together.

## Demo

For this demo, we will be getting familiar with docker logs and how are they being read by fluentd, by the end of this demo, you will be a fluentd ninja! 😁

## Setup

To help you with the setup, I’ve created [this repo](https://github.com/r1ckr/fluentd-simplified), after cloning it you will end up with the following directory structure:

    fluentd/
        ├── etc/
        │   └── fluentd.conf
        ├── log/
        │   └── kong.log
        └── output/

In output/ is where fluentd is going to write the files.

In log/kong.log we have some logs from a kong container that I have running in my laptop. Take a look at these logs, they have the docker format:

    {
        **"log"**:"2019/07/31 22:19:52 [notice] 1#0: start worker process 32\n",
        **"stream"**:"stderr",
        **"time"**:"2019-07-31T22:19:52.3754634Z"
    }

Every line of that file is a single json document, this is how docker logs are with the default driver. We will be tailing and parsing this file, it has application logs mixed with access logs. Our goal here is to get only the access logs.

etc/fluentd.conf is our fluentd configuration, take a look at it, you can see that there's an input and an output section, we will be takin a closer look to it later, first let's run the fluentd container:

## Running fluentd

    docker run -ti --rm **\
    **-v **$(**pwd**)**/etc:/fluentd/etc **\
    **-v **$(**pwd**)**/log:/var/log/ **\
    **-v **$(**pwd**)**/output:/output **\
    **fluent/fluentd:v1.10-debian-1 -c /fluentd/etc/fluentd.conf -v

Pay attention to that run command and the volumes we are mounting:

* etc/ is mounted onto the /fluentd/etc/ directory inside of the container to override fluentd default config.

* log/ onto /var/log/ ending up with /var/log/kong.log inside of the container.

* output/ onto /output to be able to see what fluentd writes to disk.

After running the container you should see a line like:

    2020-05-10 17:33:36 +0000 [info]: *#0 fluent/log.rb:327:info: fluentd worker is now running worker=0*

This means that fluentd is up and running.

Now that we know how everything is wired and fluentd is running, let’s see the config file in detail and start exploring what fluentd is doing with it:

## Fluentd config

First we have the input section

    **<source>**
      @type tail
      path "/var/log/*.log"
      tag "ninja.*"
      read_from_head true
      **<parse>**
        @type "json"
        time_format "%Y-%m-%dT%H:%M:%S.%NZ"
        time_type string
      **</parse>**
    **</source>**

Let’s take a closer look to some of that config:

* @type tail: is the type of input we want, this is very similar to tail -f

* path "/var/log/*.log": This means that it will tail any file ending with .log, each file will generate its own tag like: var.log.kong.log

* tag "ninja.*": This is going to prepend ninja. to every tag created by this source, in this case we have only one file ending up with ninja.var.log.kong.log

* read_from_head true: To consume the whole file instead of only new lines.

* <parse> section: Since every line of the docker logs is a json object, we are going to parse as json.

Then we have the output section:

    **<match** ****>**
      @type file
      path /output/example.log
      **<buffer>**
        timekey 1d
        timekey_use_utc true
        timekey_wait 1m
      **</buffer>**
    **</match>**

There are 2 important bits in this config:

* <match **>: This means that we are going to match any tag in fluentd, we only have one so far, the one created by the input plugin.

* path /output/example: This is the name of the directory where buffers are kept and the beginning of every log file, ending up with this:

    ├── example
        │   ├── buffer.b5a579308a9becefb07a20821ea3ab218.log
        │   └── buffer.b5a579308a9becefb07a20821ea3ab218.log.meta
        ├── example.20190731.log
        └── example.20200510.log

With this configuration we have a very simple input/output pipeline like so:

![](https://cdn-images-1.medium.com/max/5200/0*9mUcDLt6X9H_tGPX.png)

Now, let’s see some logs from one of the files that fluentd has created example.20200510.log:

    2020-05-10T17:04:17+00:00	ninja.var.log.kong.log	{"log":"2020/05/10 17:04:16 [warn] 35#0: *4 [lua] globalpatches.lua:47: sleep(): executing a blocking 'sleep' (0.004 seconds), context: init_worker_by_lua*\n","stream":"stderr"}
    2020-05-10T17:04:17+00:00	ninja.var.log.kong.log	{"log":"2020/05/10 17:04:16 [warn] 33#0: *2 [lua] globalpatches.lua:47: sleep(): executing a blocking 'sleep' (0.008 seconds), context: init_worker_by_lua*\n","stream":"stderr"}
    2020-05-10T17:04:17+00:00	ninja.var.log.kong.log	{"log":"2020/05/10 17:04:17 [warn] 32#0: *1 [lua] mesh.lua:86: init(): no cluster_ca in declarative configuration: cannot use node in mesh mode, context: init_worker_by_lua*\n","stream":"stderr"}
    2020-05-10T17:04:30+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:04:30 +0000] \"GET / HTTP/1.1\" 404 48 \"-\" \"curl/7.59.0\"\n","stream":"stdout"}
    2020-05-10T17:05:38+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:05:38 +0000] \"GET /users HTTP/1.1\" 401 26 \"-\" \"curl/7.59.0\"\n","stream":"stdout"}
    2020-05-10T17:06:24+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:06:24 +0000] \"GET /users HTTP/1.1\" 499 0 \"-\" \"curl/7.59.0\"\n","stream":"stdout"}

Pay attention to it, there are 3 columns in each line:

    <time of the log>    <tag of the log>    <the content of the log>

**Note:** Check that the tag has the “ninja” string plus the directory path and the file name, all separated by “.”

## Filtering the logs

Now that we have our logs working in fluentd, let’s apply some filtering to it.

So far we have implemented 2 sections of that previous command working fine, the tail -f /var/log/*.log and the > example.log, but if you take a look at the output, we have access logs mixed with application logs, now we need to implement the grep 'what I want' filter.

For this example we want only the access logs and we will discard all the other lines, please note that you should always keep all your logs, including the application logs, this is just for demonstration.

Let’s say that grepping by HTTP will give us all the access logs and will exclude the application logs, this configuration will do that:

    **<filter** ninja.var.log.kong****>**
      @type grep
      **<regexp>**
        key log
        pattern /HTTP/
      **</regexp>**
    **</filter>**

Let’s take a look at this config:

* <filter ninja.var.log.kong**>: We will filter only **tags** starting with ninja.var.log.kong.

* @type grep: The plugin type we are going to use, this is exactly like grep

* <regexp> section: Here we are grepping “HTTP” in the log key of the log content

With this configuration we have added a new block in our pipeline like so:

![](https://cdn-images-1.medium.com/max/5200/0*3y_AXGdul9JeL0RB.png)

Let’s stop and run the container again.

Now you should see something different in the output logs, there are no application logs, we have only access logs:

    2020-05-10T17:04:30+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:04:30 +0000] \"GET / HTTP/1.1\" 404 48 \"-\" \"curl/7.59.0\"\n","stream":"stdout"}
    2020-05-10T17:05:38+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:05:38 +0000] \"GET /users HTTP/1.1\" 401 26 \"-\" \"curl/7.59.0\"\n","stream":"stdout"}
    2020-05-10T17:06:24+00:00	ninja.var.log.kong.log	{"log":"172.17.0.1 - - [10/May/2020:17:06:24 +0000] \"GET /users HTTP/1.1\" 499 0 \"-\" \"curl/7.59.0\"\n","strea

## Parsing Access Logs

Just to practice, let’s add a parser plugin to extract useful information from the access logs.

Use this config after the grep filter:

    **<filter** ninja.var.log.kong** **>**
      @type parser
      key_name log
      **<parse>**
        @type nginx
      **</parse>**
    **</filter>**

Let’s review that config:

* <filter ninja.var.log.kong**>: We will parse any **tags** starting with ninja.var.log.kong same as before

* The type of filter is parser

* We are going to parse the log key of the log content

* Since those are nginx access logs, we are going to use the parser @type nginx

This is how our pipeline is looking now:

![](https://cdn-images-1.medium.com/max/5200/0*L6h6o0cbIgL2vsyS.png)

Let’s run that docker command again and see the logs. The Kong access logs should be looking like this:

    2020-05-10T17:04:30+00:00	ninja.var.log.kong.log	{"remote":"172.17.0.1","host":"-","user":"-","method":"GET","path":"/","code":"404","size":"48","referer":"-","agent":"curl/7.59.0","http_x_forwarded_for":""}

That is the first access log from the previous logs, now the log content is completely different, our keys have changed from log and stream, to remote, host, user, method, path, code, size, referer, agent and http_x_forwarded_for. If we were to inject this into Elasticsearch we will be able to filter by method=GET or any other combination.

You can go even further and apply [geoip](https://docs.fluentd.org/filter/geoip) to the remote field to extract the countries of the IPs hitting our API, but I'll leave that to you, cause you are now a fluentd ninja.

## Recap

Now we know how to setup fluentd with docker to read any file, we can identify the different sections in the configuration, we were able to translate a linux command to it and finally we were able to see a glimpse of parsing some logs to extract useful information from it.

I hope you were able to follow through with me on fluentd and it has become a simpler tool to use. Enjoy logging!
